# 线性回归

给定数据集 $\mathbf{X} \in \mathbb{R}^{d \times n}$ ，令$\vec{x}_i \in \mathbb{R}^d (0 \lt i \le n)$表示$\mathbf{X}$中第$i$个元素。

设数据集$\mathbf{X}$的目标值为$\vec{y} \in \mathbb{R}^n$，令$y_i \in \mathbb{R} (0 \lt i \le n)$表示$\vec{y}$中第$i$个元素。

## 1. 线性模型

在**线性回归**中，因变量$y$可以表示为自变量$\vec{x}$中元素的加权和再加上一些噪声（偏置），这些噪声通常遵循正态分布。

当自变量$\vec{x}$包含$d$个**特征**时，我们将预测结果（通常使用“尖角”符号表示的估计值）表示为：

$$ \hat{y}=w_1x_1 + w_2x_2 + \cdots + b $$

或用点积的形式简洁地表达模型：

$$ \hat{y} = \vec{w}^\top \vec{x} + b $$

其中$\vec{x} \in \mathbb{R}^d$表示特征，$\vec{w} \in \mathbb{R}^d$表示权重，$b$表示偏置。

对于特征集合$\mathbf{X} \in \mathbb{R}^{d \times n}$，预测值$\hat{ \vec{y} } \in \mathbb{R}^n$可以通过矩阵向量积表示为：

$$ \hat{ \vec{y} } = \mathbf{X} \vec{w} + b $$

式中的加法用到了广播机制。

## 2. 损失函数

**损失函数**用于评估实际值和预测值之间的差距。通常选择非负数作为损失，且数值越小表示损失越小；完美预测时，损失数值为$0$。损失函数是权重与偏置的函数，与数据集无关：

$$ l = l(\vec{w}, b) $$

**平方误差函数**是一种最常见的损失函数：

$$ l_i(\vec{w}, b) = \frac{1}{2} (\hat{y}_i - y_i)^2 $$

其中，常数$\frac{1}{2}$不会影响损失函数的效果，其作用是使函数求导后的形式更为简洁。

为了度量模型在整个数据集上的质量，我们需计算在数据集中$n$个样本上的平均值（也等价于求和）。

$$
\begin{align}
L(\vec{w}, b)
&= \frac{1}{n} \sum\limits_{i=1}^n{ l_i(\vec{w}, b) } \\\\
&= \frac{1}{2n} \sum\limits_{i=1}^n (y_i - \vec{w}^\top \vec{x}_i - b)^2
\end{align}
$$

在训练模型时，我们希望寻找一组参数$(\vec{w}^\ast, b^\ast)$，使得数据集上的总损失最小，式如下：

$$ (\vec{w}^\ast, b^\ast) = \arg\min_{\vec{w}, b} L(\vec{w}, b) $$

### 解析解

先假设$b = 0$，则根据损失函数公式有：

$$ L(\vec{w}, b) = \frac{1}{2n} \| \vec{y} - \mathbf{X}\vec{w} \|^2, $$

要使其最小，则

$$ \vec{w}^\ast = (X^\top X)^{-1} X^\top y. $$

再引入$b$，求损失函数关于$b$的偏导数得

$$ \frac{ \partial{L(\vec{w}, b)} }{ \partial{b} } = b - \frac{1}{n} \sum\limits_{i=1}^n (y_i - \vec{w}^\top\vec{x}_i), $$

当

$$ b^\ast = \frac{1}{n} \sum\limits\_{i=1}^n (y_i - {\vec{w}^\ast}^\top\vec{x}_i) $$

时，损失函数$L(\vec{w}, b)$最小。

## 3. 梯度下降

**梯度下降**通过不断地在损失函数递减的方向上更新参数来降低误差。其最简单的用法是计算损失函数关于模型参数的导数（梯度）。但实际中的执行可能会非常慢：因为在每一次更新参数之前，我们必须遍历整个数据集。因此，我们通常会在每次需要计算更新的时候随机抽取一小批样本，这种变体叫做**小批量随机梯度下降**。

在每次迭代中，首先随机抽取一个小批量数据集$\mathcal{B} \subseteq \mathbf{X}$。然后计算该数据集的平均损失关于模型参数的导数（梯度）。最后从当前参数值中减去一定系数的该梯度值，该系数又称**学习率**，记为$\eta (\eta \gt 0)$。该过程可以表示为：

$$
\begin{align}
\vec{w} &\leftarrow \vec{w} - \frac{\eta}{\|\mathcal{B}\|} \sum\limits\_{i \in \mathcal{B} } \frac{\partial l_i(\vec{w}, b)}{\partial \vec{w} } \\\\
&= \vec{w} - \frac{\eta}{\|\mathcal{B}\|} \sum\limits\_{i \in \mathcal{B} } \vec{x}_i (\vec{w}^\top \vec{x}_i + b - y_i) \\\\
b &\leftarrow b - \frac{\eta}{\|\mathcal{B}\|} \sum\limits\_{i \in \mathcal{B} } \frac{\partial l_i(\vec{w}, b)}{\partial b} \\\\
&= b - \frac{\eta}{\|\mathcal{B}\|} \sum\limits\_{i \in \mathcal{B} } (\vec{w}^\top \vec{x}_i + b - y_i)
\end{align}
$$

当满足一定条件或达到一定迭代次数时，将此时模型参数的估计值记为$(\hat{ \vec{w} }, \hat{b})$。该算法会使估计值向最小值缓慢收敛，但不能在有限步骤内精准计算出最小值。

## 4. 均方损失

**正态分布**的概率密度为：

$$ p(\epsilon) = \frac{1}{ \sqrt{2 \pi \sigma^2} } e^{- \frac{1}{2 \sigma^2} (\epsilon - \mu)^2} $$

**均方误差损失函数**（简称均方损失）可以用于线性回归的一个原因是，我们假设了观测中包含噪声。

$$ y = \vec{w}^\top \vec{x} + b + \epsilon $$

其中，噪声服从正态分布，$\epsilon \sim \mathcal{N}(0, \sigma^2)$。将$\epsilon$带入正态分布概率密度函数可得到通过给定的$\vec{x}$观测到特定$y$的似然：

$$ p(y | \vec{x}) = \frac{1}{ \sqrt{2 \pi \sigma^2} } e^{- \frac{1}{2 \sigma^2} (y - \vec{w}^\top \vec{x} - b)^2 } $$

根据极大似然估计法，参数$(\vec{w}, b)$的最优值是使整个数据集的似然最大的值。令似然函数为：

$$ p(\vec{y} | \mathbf{X}) = \prod\limits\_{i=1}^n{p(y_i|\vec{x}_i)} $$

两边取负对数得：

$$
\begin{align}
-\log{ p(\vec{y} | \mathbf{X}) } =
& \frac{n}{2} \log{(2 \pi \sigma^2)} + \\\\
& \frac{1}{2 \sigma^2} \sum\limits_{i=1}^n (y_i - \vec{w}^\top\vec{x}_i - b)^2
\end{align}
$$

假设$\sigma$是某个固定常数，那么上式就可以忽略第一项，以及第二项的常数部分。而其余部分和均方误差是一样的。因此，在高斯噪声的假设下，最小化均方误差等价于对线性模型的极大似然估计：

$$ L(\vec{w}, b) = -\log{ p(\vec{y} | \mathbf{X}) } $$
